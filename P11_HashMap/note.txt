1.哈希函数：
1）输入域无穷，输出域有限
2）相同输入有相同输出
3）不同输入，也可能有相同输出
4）均匀性和离散性


2.设计RandomPool结构：
设计一种结构，要求实现下面三个功能：
insert(key):将某个key加入到该结构，做到不重复加入
delete(key):将在结构中的key删除
getRandom():等概率随机返回一个key

要求：三个方法时间为O(1)
方法：建立两个HashMap<key,Index>和HashMap<Index,key>,一个int size，将key输入和当时的size输入两个Hashmap。
对于getRandom，随机返回hashmap中的一个
对于delete，删除原本index上的数据，拿最后一条数据补上，并且删除最后一条数据，更新size




3.布隆过滤器：可以减少内存使用，但是失误率不能避免，没有删除行为
解决黑名单问题，不能让用户访问这些黑名单，但是允许一定程度的失误率（不是黑名单的东西，当时误当成是黑名单，不让访问）
（位图：一个单位一比特的数组）Code05

方法：建立黑名单：建立一个大小为m bit 的位图，将url调用k个哈希函数，生成k个值，k个值去%m，在位图中涂黑这些区域，遍历url，涂黑所有生成的区域
查询： 将新的url调用k个哈希函数，生成k个值，在位图中查询这k个值，如果全都为黑，那么不允许访问，否则，允许访问
决定失误率的因素：位图大小 m， 哈希函数个数k根据位图大小m和要检验的总数综合考虑

n = 样本量
p = 失误率
单样本大小与布隆过滤器无关

大小 m = -(n * lnp)/(ln2*ln2) 向上取整
哈希函数个数 k = ln2 * m / n  向上取整

实际失误率 p = (1- e的(-nk/m))的k次方




4.一致性哈希原理：讨论数据服务器怎么组织的问题，保证在增加服务器的情况下，数据迁移代价不高
哈希值怎么选择？ 要设置种类很多，高、中、低频都有一定数量的进行划分（bad：国家名，男女。。。在如果有三台服务器的情况下，男女只能存两个，必有一个访问极低）

问题一：当服务器很少的时候，怎么把数据均分（数据怎么知道存哪台机器）

问题二：当增加服务器时，负载马上不均衡

可以使用虚拟节点技术来解决以上问题
设服务器m1、m2、m3各有1000个虚拟节点，就可以将数据分成3000份，因为哈希函数的均匀性，随机取一定量的数据，访问m1、m2、m3的次数大体相同
在这种情况下，加入m4，m4要从1、2、3拿大约等量的数据，删去m4时，1、2、3要收取等量的数据


还可以管理负载：
对于能力更强的机器，可以增加更多的虚拟节点，让其管理更多数据




